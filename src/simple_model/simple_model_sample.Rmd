---
title: "Simple Model Example"
author: "J. Thompson"
date: "September 4, 2016"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
library(caret)
library(plyr)
library(dplyr)
library(xgboost)
library(ranger)
library(nnet)
library(Metrics)

knitr::opts_chunk$set(echo = TRUE)

ROOT.DIR <- "../.." #getwd()

DATA.DIR <- paste(ROOT.DIR,"data",sep="/")
```
This is an example of a simple GBM model.  A 5-fold cross-validation strategy is
used for model fitting.  The model is fitted to using the log of SalePrice as
the target variable.

# Data Preparation

## Retrieve Data
```{r InputData}
train.raw <- read.csv(file.path(DATA.DIR,"train.csv"),stringsAsFactors = FALSE)

test.raw <- read.csv(file.path(DATA.DIR,"test.csv"), stringsAsFactors = FALSE)

```

## Initial Data Profile

Feature selection is based on an [earlier Boruta](https://www.kaggle.com/jimthompson/house-prices-advanced-regression-techniques/boruta-feature-importance-analysis) feature importance analysis.

```{r SetupForDataPrep}
# incorporate results of Boruta analysis
CONFIRMED_ATTR <- c("MSSubClass","MSZoning","LotArea","LotShape","LandContour","Neighborhood",
                    "BldgType","HouseStyle","OverallQual","OverallCond","YearBuilt",
                    "YearRemodAdd","Exterior1st","Exterior2nd","MasVnrArea","ExterQual",
                    "Foundation","BsmtQual","BsmtCond","BsmtFinType1","BsmtFinSF1",
                    "BsmtFinType2","BsmtUnfSF","TotalBsmtSF","HeatingQC","CentralAir",
                    "X1stFlrSF","X2ndFlrSF","GrLivArea","BsmtFullBath","FullBath","HalfBath",
                    "BedroomAbvGr","KitchenAbvGr","KitchenQual","TotRmsAbvGrd","Functional",
                    "Fireplaces","FireplaceQu","GarageType","GarageYrBlt","GarageFinish",
                    "GarageCars","GarageArea","GarageQual","GarageCond","PavedDrive","WoodDeckSF",
                    "OpenPorchSF","Fence")

TENTATIVE_ATTR <- c("Alley","LandSlope","Condition1","RoofStyle","MasVnrType","BsmtExposure",
                    "Electrical","EnclosedPorch","SaleCondition")

REJECTED_ATTR <- c("LotFrontage","Street","Utilities","LotConfig","Condition2","RoofMatl",
                   "ExterCond","BsmtFinSF2","Heating","LowQualFinSF","BsmtHalfBath",
                   "X3SsnPorch","ScreenPorch","PoolArea","PoolQC","MiscFeature","MiscVal",
                   "MoSold","YrSold","SaleType")

PREDICTOR_ATTR <- c(CONFIRMED_ATTR,TENTATIVE_ATTR,REJECTED_ATTR)

# Determine data types in the data set
data_types <- sapply(PREDICTOR_ATTR,function(x){class(train.raw[[x]])})
unique_data_types <- unique(data_types)

# Separate attributes by data type
DATA_ATTR_TYPES <- lapply(unique_data_types,function(x){ names(data_types[data_types == x])})
names(DATA_ATTR_TYPES) <- unique_data_types


# create folds for training
set.seed(13)
data_folds <- createFolds(train.raw$SalePrice, k=5)

```


## Create Feature Sets
```{r PrepareFeatureSets}
# Feature Set 1 - Boruta Confirmed Attributes
prepL0FeatureSet1 <- function(df) {
    id <- df$Id
    if (class(df$SalePrice) != "NULL") {
        y <- log(df$SalePrice)
    } else {
        y <- NULL
    }
    
    
    predictor_vars <- CONFIRMED_ATTR
    
    predictors <- df[predictor_vars]
    
    # for numeric set missing values to -1 for purposes
    num_attr <- intersect(predictor_vars,DATA_ATTR_TYPES$integer)
    for (x in num_attr){
      predictors[[x]][is.na(predictors[[x]])] <- -1
    }

    # for character  atributes set missing value
    char_attr <- intersect(predictor_vars,DATA_ATTR_TYPES$character)
    for (x in char_attr){
      predictors[[x]][is.na(predictors[[x]])] <- "*MISSING*"
      predictors[[x]] <- factor(predictors[[x]])
    }
    
    return(list(id=id,y=y,predictors=predictors))
}

L0FeatureSet1 <- list(train=prepL0FeatureSet1(train.raw),
                    test=prepL0FeatureSet1(test.raw))

```

## Model Training
### Helper Function For Training 
```{r}
#train model on one data fold
trainOneFold <- function(this_fold,feature_set) {
    # get fold specific test data
    test.data <- list()
    test.data$predictors <- feature_set$train$predictors[this_fold,]
    test.data$ID <- feature_set$train$id[this_fold]
    test.data$y <- feature_set$train$y[this_fold]
    
    # get training data for specific fold
    train.data <- list()
    train.data$predictors <- feature_set$train$predictors[-this_fold,]
    train.data$y <- feature_set$train$y[-this_fold]

    
    set.seed(825)
    time.data <- system.time(fitted_mdl <- do.call(train,c(list(x=train.data$predictors,
                                                             y=train.data$y),
                                                        CARET.TRAIN.PARMS,
                                                        MODEL.SPECIFIC.PARMS,
                                                        CARET.TRAIN.OTHER.PARMS)))
    time.data
    
    
    yhat <- predict(fitted_mdl,newdata = test.data$predictors,type = "raw")
    
    score <- rmse(test.data$y,yhat)
    
    ans <- list(fitted_mdl=fitted_mdl,
                score=score,
                features=data.frame(ID=test.data$ID,yhat=yhat,y=test.data$y))
    
    return(ans)
    
}

# make prediction from a model fitted to one fold
makeOneFoldTestPrediction <- function(this_fold,feature_set) {
    fitted_mdl <- this_fold$fitted_mdl
    
    yhat <- predict(fitted_mdl,newdata = feature_set$test$predictors,type = "raw")
    
    return(yhat)
}
```


### gbm Model
```{r ,eval=TRUE,message=FALSE,warning==FALSE}
# set caret training parameters
CARET.TRAIN.PARMS <- list(method="gbm")   

CARET.TUNE.GRID <-  expand.grid(n.trees=100, 
                                interaction.depth=10, 
                                shrinkage=0.1,
                                n.minobsinnode=10)

MODEL.SPECIFIC.PARMS <- list(verbose=0) #NULL # Other model specific parameters

# model specific training parameter
CARET.TRAIN.CTRL <- trainControl(method="none",
                                 verboseIter=FALSE,
                                 classProbs=FALSE)

CARET.TRAIN.OTHER.PARMS <- list(trControl=CARET.TRAIN.CTRL,
                           tuneGrid=CARET.TUNE.GRID,
                           metric="RMSE")


time.data <- system.time(mdl_set <- llply(data_folds,trainOneFold,
                                     L0FeatureSet1))


test_y <- do.call(c,lapply(mdl_set,function(x){x$features$y}))
test_yhat <- do.call(c,lapply(mdl_set,function(x){x$features$yhat}))
rmse(test_y,test_yhat)
cat("Average test fold rmse:",mean(do.call(c,lapply(mdl_set,function(x){x$score}))))


# create test submission.
# A prediction is made by averaging the predictions made by using the models
# fitted for each fold.
test_yhat <- do.call(cbind,lapply(mdl_set,makeOneFoldTestPrediction,L0FeatureSet1))
test_yhat <- apply(test_yhat,1,mean)

submission <- cbind(Id=L0FeatureSet1$test$id,SalePrice=exp(test_yhat))

write.csv(submission,file="gbm_sumbission.csv",row.names=FALSE)


```


